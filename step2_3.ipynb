{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bipartite Graph: 89302 nodes, 99197 edges\n",
      "User-User Graph: 77604 nodes, 3437368 edges\n",
      "Business-Business Graph: 6531 nodes, 42364 edges\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the merged dataset\n",
    "data = pd.read_csv('Merged_Data_Part0.csv')  # Replace with your actual merged CSV file name\n",
    "\n",
    "# Create a Bipartite Graph\n",
    "def create_bipartite_graph(data):\n",
    "    \"\"\"\n",
    "    Create a bipartite graph where nodes are users and businesses, and edges are reviews.\n",
    "    Edges are weighted by review scores, with timestamps as edge attributes.\n",
    "    \"\"\"\n",
    "    B = nx.Graph()\n",
    "\n",
    "    # Add user and business nodes\n",
    "    B.add_nodes_from(data['user_id'], bipartite=0)  # Layer 0: Users\n",
    "    B.add_nodes_from(data['business_id'], bipartite=1)  # Layer 1: Businesses\n",
    "\n",
    "    # Add edges (user-business interactions) with weights and attributes\n",
    "    for _, row in data.iterrows():\n",
    "        B.add_edge(\n",
    "            row['user_id'], \n",
    "            row['business_id'], \n",
    "            weight=row['stars_x'],  # Weight by review stars\n",
    "            date=row['date']       # Add review date as an attribute\n",
    "        )\n",
    "    \n",
    "    return B\n",
    "\n",
    "# Create bipartite graph\n",
    "bipartite_graph = create_bipartite_graph(data)\n",
    "print(f\"Bipartite Graph: {bipartite_graph.number_of_nodes()} nodes, {bipartite_graph.number_of_edges()} edges\")\n",
    "\n",
    "# Project Graphs\n",
    "def project_graphs(bipartite_graph, data):\n",
    "    \"\"\"\n",
    "    Create user-user and business-business projected graphs based on shared interactions.\n",
    "    Weights represent the strength of the connection (e.g., number of shared reviews).\n",
    "    \"\"\"\n",
    "    # Create user-user projection\n",
    "    user_nodes = {n for n, d in bipartite_graph.nodes(data=True) if d['bipartite'] == 0}\n",
    "    user_graph = nx.Graph()\n",
    "    \n",
    "    for business in data['business_id'].unique():\n",
    "        # Get users who reviewed the same business\n",
    "        reviewers = data[data['business_id'] == business]['user_id'].unique()\n",
    "        # Add edges between users who reviewed the same business\n",
    "        for u1, u2 in combinations(reviewers, 2):\n",
    "            if user_graph.has_edge(u1, u2):\n",
    "                user_graph[u1][u2]['weight'] += 1\n",
    "            else:\n",
    "                user_graph.add_edge(u1, u2, weight=1)\n",
    "    \n",
    "    # Create business-business projection\n",
    "    business_nodes = {n for n, d in bipartite_graph.nodes(data=True) if d['bipartite'] == 1}\n",
    "    business_graph = nx.Graph()\n",
    "    \n",
    "    for user in data['user_id'].unique():\n",
    "        # Get businesses reviewed by the same user\n",
    "        businesses = data[data['user_id'] == user]['business_id'].unique()\n",
    "        # Add edges between businesses reviewed by the same user\n",
    "        for b1, b2 in combinations(businesses, 2):\n",
    "            if business_graph.has_edge(b1, b2):\n",
    "                business_graph[b1][b2]['weight'] += 1\n",
    "            else:\n",
    "                business_graph.add_edge(b1, b2, weight=1)\n",
    "    \n",
    "    return user_graph, business_graph\n",
    "\n",
    "# Create projected graphs\n",
    "user_graph, business_graph = project_graphs(bipartite_graph, data)\n",
    "\n",
    "print(f\"User-User Graph: {user_graph.number_of_nodes()} nodes, {user_graph.number_of_edges()} edges\")\n",
    "print(f\"Business-Business Graph: {business_graph.number_of_nodes()} nodes, {business_graph.number_of_edges()} edges\")\n",
    "\n",
    "# Save graphs for future use\n",
    "def save_graph_pickle(graph, filename):\n",
    "    \"\"\"Save a NetworkX graph to a file using pickle.\"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "save_graph_pickle(bipartite_graph, \"bipartite_graph.pkl\")\n",
    "save_graph_pickle(user_graph, \"user_graph.pkl\")\n",
    "save_graph_pickle(business_graph, \"business_graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.44274765253067017\n",
      "Epoch 2, Loss: 0.2764238119125366\n",
      "Epoch 3, Loss: 0.1794947236776352\n",
      "Epoch 4, Loss: 0.14455430209636688\n",
      "Epoch 5, Loss: 0.08563492447137833\n",
      "Epoch 6, Loss: 0.1112779974937439\n",
      "Epoch 7, Loss: 0.10334470868110657\n",
      "Epoch 8, Loss: 0.07464113086462021\n",
      "Epoch 9, Loss: 0.06265848875045776\n",
      "Epoch 10, Loss: 0.04954126477241516\n",
      "Epoch 11, Loss: 0.045813336968421936\n",
      "Epoch 12, Loss: 0.056700557470321655\n",
      "Epoch 13, Loss: 0.05220134183764458\n",
      "Epoch 14, Loss: 0.03867248818278313\n",
      "Epoch 15, Loss: 0.03051736205816269\n",
      "Epoch 16, Loss: 0.023138388991355896\n",
      "Epoch 17, Loss: 0.03132222220301628\n",
      "Epoch 18, Loss: 0.031078534200787544\n",
      "Epoch 19, Loss: 0.02639014832675457\n",
      "Epoch 20, Loss: 0.021570831537246704\n",
      "Epoch 21, Loss: 0.015716014429926872\n",
      "Epoch 22, Loss: 0.017590228468179703\n",
      "Epoch 23, Loss: 0.017857061699032784\n",
      "Epoch 24, Loss: 0.017856402322649956\n",
      "Epoch 25, Loss: 0.013402334414422512\n",
      "Epoch 26, Loss: 0.010719970799982548\n",
      "Epoch 27, Loss: 0.012814458459615707\n",
      "Epoch 28, Loss: 0.013085906393826008\n",
      "Epoch 29, Loss: 0.01197129301726818\n",
      "Epoch 30, Loss: 0.008670716546475887\n",
      "Epoch 31, Loss: 0.009337527677416801\n",
      "Epoch 32, Loss: 0.009520101360976696\n",
      "Epoch 33, Loss: 0.008342546410858631\n",
      "Epoch 34, Loss: 0.008695452474057674\n",
      "Epoch 35, Loss: 0.007155065890401602\n",
      "Epoch 36, Loss: 0.0058054495602846146\n",
      "Epoch 37, Loss: 0.006952349562197924\n",
      "Epoch 38, Loss: 0.006840184330940247\n",
      "Epoch 39, Loss: 0.005850876681506634\n",
      "Epoch 40, Loss: 0.004614299163222313\n",
      "Epoch 41, Loss: 0.005409828387200832\n",
      "Epoch 42, Loss: 0.004895305726677179\n",
      "Epoch 43, Loss: 0.004334721248596907\n",
      "Epoch 44, Loss: 0.004362532868981361\n",
      "Epoch 45, Loss: 0.005878242664039135\n",
      "Epoch 46, Loss: 0.005023754667490721\n",
      "Epoch 47, Loss: 0.004430586937814951\n",
      "Epoch 48, Loss: 0.00411207415163517\n",
      "Epoch 49, Loss: 0.005120375193655491\n",
      "Epoch 50, Loss: 0.004048725124448538\n",
      "Epoch 51, Loss: 0.004827412776648998\n",
      "Epoch 52, Loss: 0.004180957563221455\n",
      "Epoch 53, Loss: 0.0037023837212473154\n",
      "Epoch 54, Loss: 0.0034660182427614927\n",
      "Epoch 55, Loss: 0.005107950419187546\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Add prefixes to IDs to ensure uniqueness\n",
    "data['user_id_prefixed'] = 'u_' + data['user_id'].astype(str)\n",
    "data['business_id_prefixed'] = 'b_' + data['business_id'].astype(str)\n",
    "\n",
    "# Encode nodes as integers for PyTorch Geometric\n",
    "user_nodes = data['user_id_prefixed'].unique()\n",
    "business_nodes = data['business_id_prefixed'].unique()\n",
    "\n",
    "node_encoder = LabelEncoder()\n",
    "all_nodes = np.concatenate([user_nodes, business_nodes])\n",
    "node_encoder.fit(all_nodes)\n",
    "\n",
    "data['user_encoded'] = node_encoder.transform(data['user_id_prefixed'])\n",
    "data['business_encoded'] = node_encoder.transform(data['business_id_prefixed'])\n",
    "\n",
    "# Create PyTorch Geometric graph\n",
    "edge_index = torch.tensor(data[['user_encoded', 'business_encoded']].values.T, dtype=torch.long)\n",
    "edge_attr = torch.tensor(data['stars_x'].values, dtype=torch.float)\n",
    "\n",
    "# Define PyTorch Geometric Data object\n",
    "graph_data = Data(edge_index=edge_index)\n",
    "\n",
    "# Define GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Set parameters\n",
    "num_nodes = len(all_nodes)\n",
    "embedding_dim = 64\n",
    "x = torch.eye(num_nodes)  # Initialize node features as identity matrix\n",
    "\n",
    "model = GraphSAGE(num_nodes, 128, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train GraphSAGE\n",
    "model.train()\n",
    "for epoch in range(55):\n",
    "    optimizer.zero_grad()\n",
    "    embeddings = model(x, edge_index)\n",
    "    loss = torch.mean(embeddings.norm(dim=1))  # Add a dummy loss (replace as needed)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE embeddings saved to graphsage_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings\n",
    "embeddings = embeddings.detach().numpy()\n",
    "embedding_df = pd.DataFrame(embeddings, index=node_encoder.inverse_transform(range(num_nodes)))\n",
    "embedding_df.to_csv(\"graphsage_embeddings.csv\", index_label=\"node\")\n",
    "print(\"GraphSAGE embeddings saved to graphsage_embeddings.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
